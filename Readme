# install_k8s_arch_linux
How to install kubernetes barremetal on arch linux and create one cluster

1- Deployment tools
[run] sudo pacman -S kubeadm kubectl kubelet docker

2- Setup forwarding IPv4 and letting iptables see bridged traffic
[run] sudo vim /etc/modules-load.d/k8s.conf
[add] :
overlay
br_netfilter

[run] sudo vim /etc/sysctl.d/k8s.conf
[add] :
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1

Apply them without rebooting :
[run] sysctl --system
[run] sudo systemctl enable containerd.service
[run] sudo systemctl start containerd.service

3- Disable swap
[run] sudo swapoff -a
[run] sudo sed -i '/ swap / s/^/#/' /etc/fstab

4- Choose cgroup drive
[run] sudo containerd
[run] sudo mkdir /etc/containerd
[run] sudo touch /etc/containerd/config.toml
[remplace and run] sudo chown user:usergroup /etc/containerd/config.tom
[run] containerd config default > /etc/containerd/config.toml
[run] vim /etc/containerd/config.toml
[search and replace] :
SystemdCgroup = true

5- Restart containerd.service
[run] sudo systemctl restart containerd.service

6- Start docker
[run] sudo systemctl daemon-reload
[run] sudo systemctl enable docker
[run] sudo systemctl start docker

7- Configure crictl to use containerd cri
[run] sudo crictl config --set runtime-endpoint=unix:///run/containerd/containerd.sock --set image-endpoint=unix:///run/containerd/containerd.sock

8- Create virtual interface to use as controler ip
[run] sudo pacman -S net-tools
[read this repository and create veth] https://github.com/Shermine237/script-init-k8s.git

9- Init kubernetes
[run] sudo systemctl enable kubelet
[run] sudo kubeadm init --cri-socket unix:///run/containerd/containerd.sock --pod-network-cidr=192.168.0.0/16 --apiserver-advertise-address 192.168.100.100

10- Initialising
[run] mkdir -p $HOME/.kube
[run] sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[run] sudo chown $(id -u):$(id -g) $HOME/.kube/config
[run] export KUBECONFIG=$HOME/.kube/config

11- Config network plugin (project calico)
[run] curl -L https://github.com/projectcalico/calico/releases/download/v3.28.1/calicoctl-linux-amd64 -o calicoctl
[run] sudo chmod +x calicoctl 
[run] sudo mv calicoctl /usr/bin

[run] kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/tigera-operator.yaml
[run] kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/custom-resources.yaml
[run] watch kubectl get pods -n calico-system
Wait until each pod has the STATUS of Running

12- Remove the taints on the control plane so that you can schedule pods on it
[run] kubectl taint nodes --all node-role.kubernetes.io/control-plane-
[run] kubectl taint nodes --all node-role.kubernetes.io/control-plane-
[run] kubectl taint nodes --all node-role.kubernetes.io/master-

13- Confirm that you now have a node in your cluster with the following command (state running)
[run] kubectl get nodes -o wide





